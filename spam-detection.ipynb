{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "european-aruba",
   "metadata": {},
   "source": [
    "# Spam detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "close-composition",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import json\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import email\n",
    "import urllib\n",
    "import base64\n",
    "import string\n",
    "import quopri\n",
    "\n",
    "import seaborn as sns\n",
    "import sklearn.utils\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import words\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline    \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import precision_score, recall_score, plot_confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ultimate-raise",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "armed-anthony",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get paths of all spam and ham files\n",
    "# path change might be needed if run on linux\n",
    "\n",
    "# get list of all spam files (spamassassin)\n",
    "spam_path_spamassassin = \".\\\\datasets\\\\spamassassin\\\\spam\"\n",
    "spam_files_spamassassin = [join(spam_path_spamassassin, f) for f in listdir(spam_path_spamassassin) if isfile(join(spam_path_spamassassin, f))]\n",
    "\n",
    "# get list of all valid files (spamassassin)\n",
    "ham_path_spamassassin = \".\\\\datasets\\\\spamassassin\\\\ham\"\n",
    "ham_files_spamassassin = [join(ham_path_spamassassin, f) for f in listdir(ham_path_spamassassin) if isfile(join(ham_path_spamassassin, f))]\n",
    "\n",
    "# get list of all spam files (trec07p)\n",
    "spam_path_trec07p = \".\\\\datasets\\\\trec07p\\\\spam\"\n",
    "spam_files_trec07p = [join(spam_path_trec07p, f) for f in listdir(spam_path_trec07p) if isfile(join(spam_path_trec07p, f))]\n",
    "\n",
    "# get list of all valid files (trec07p)\n",
    "ham_path_trec07p = \".\\\\datasets\\\\trec07p\\\\ham\"\n",
    "ham_files_trec07p = [join(ham_path_trec07p, f) for f in listdir(ham_path_trec07p) if isfile(join(ham_path_trec07p, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "given-campus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all spam files, combine all ham files\n",
    "all_spam = spam_files_spamassassin + spam_files_trec07p\n",
    "all_ham = ham_files_spamassassin + ham_files_trec07p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "armed-border",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_email(email_raw):\n",
    "    email_payload = email_raw.get_payload()\n",
    "    \n",
    "    email_body = \"\"\n",
    "\n",
    "    if isinstance(email_payload, list):\n",
    "        for part in email_payload:\n",
    "            email_body += str(parse_email(part))\n",
    "\n",
    "        return email_body\n",
    "    else:\n",
    "        if \"Content-Type\" in email_raw:\n",
    "            if \"text/html\" in email_raw[\"Content-Type\"].lower() or \"text/plain\" in email_raw[\"Content-Type\"].lower(): # only parse content of type \"text/html\" and \"text/plain\"\n",
    "                if \"Content-Transfer-Encoding\" in email_raw:\n",
    "                    if email_raw[\"Content-Transfer-Encoding\"].lower() == \"base64\": # check if its base64 encoded\n",
    "                        try:\n",
    "                            return str(base64.b64decode(email_payload))\n",
    "                        except:       # if the decoding did not work\n",
    "                            return \"\" # just return an empty string\n",
    "                    elif email_raw[\"Content-Transfer-Encoding\"].lower() == \"quoted-printable\":\n",
    "                        try:\n",
    "                            email_payload = ''.join(filter(lambda x: x in string.printable, email_payload))\n",
    "                            return str(quopri.decodestring(email_payload))\n",
    "                        except:       # if the decoding did not work\n",
    "                            return \"\" # just return an empty string\n",
    "                    else:\n",
    "                        return email_payload\n",
    "                else:\n",
    "                    return email_payload\n",
    "        elif email_raw.get_default_type() == \"text/plain\":\n",
    "            # If the there is no \"Content-Type\" and the default type is \"text/plain\"\n",
    "            return email_payload\n",
    "        else:\n",
    "            return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "liked-pharmacy",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_dataset(dataset, is_spam):\n",
    "    rows = []\n",
    "    parser = email.parser.BytesParser()\n",
    "    re_email = re.compile(\"[\\w.-]+@[\\w.-]+.[\\w.-]+\", re.UNICODE)\n",
    "\n",
    "    for mail in dataset:\n",
    "        with open(mail, \"rb\") as f:\n",
    "\n",
    "            email_raw = parser.parse(f)\n",
    "\n",
    "            subject_mail = email_raw['subject']\n",
    "            from_mail = email_raw['From']\n",
    "            if from_mail != None:\n",
    "                try:\n",
    "                    from_mail = re_email.search(str(from_mail)).group()\n",
    "                except AttributeError:\n",
    "                    print(\"Could not parse email 'from' header\")\n",
    "                    from_mail = None\n",
    "            \n",
    "            try:\n",
    "                email_payload = parse_email(email_raw)\n",
    "            except Exception as e:\n",
    "                print(\"[-] Unknown email parse error\")\n",
    "                \n",
    "            if email_payload == None:\n",
    "                print(\"Could not parse email body\")\n",
    "            \n",
    "            if email_payload == None or len(email_payload) == 0:\n",
    "                email_payload = \"0\"\n",
    "\n",
    "            urls = re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[-$_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', email_payload)\n",
    "            domains = []\n",
    "            if len(urls) == 0:\n",
    "                urls = 0\n",
    "            else:\n",
    "                for url in urls:\n",
    "                    try:\n",
    "                        domains.append(re.sub(\":\\d+\", \"\", urllib.parse.urlparse(url).netloc))\n",
    "                    except:\n",
    "                        print(\"Could not parse domain\") \n",
    "                \n",
    "            if len(domains) == 0:\n",
    "                domains = 0\n",
    "\n",
    "            if BeautifulSoup(email_payload.encode(\"utf-8\"), \"html.parser\").find():\n",
    "                cleantext = BeautifulSoup(email_payload, \"html.parser\").text\n",
    "            else:\n",
    "                cleantext = email_payload\n",
    "                \n",
    "            clean_data = cleantext\n",
    "            clean_data.replace(\"\\n\", \" \")\n",
    "\n",
    "            rows.append([is_spam, cleantext, subject_mail, from_mail, urls, domains])\n",
    "            \n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "signal-married",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_spam_files = all_spam\n",
    "model_ham_files = all_ham "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "collaborative-romania",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not parse email 'from' header\n",
      "Could not parse email 'from' header\n",
      "Could not parse email 'from' header\n",
      "Could not parse email 'from' header\n",
      "Could not parse email 'from' header\n",
      "Could not parse email 'from' header\n",
      "Could not parse email 'from' header\n",
      "Could not parse email 'from' header\n",
      "Could not parse email 'from' header\n",
      "Could not parse email 'from' header\n",
      "Could not parse email body\n",
      "Could not parse email 'from' header\n",
      "Could not parse email 'from' header\n",
      "Could not parse email 'from' header\n",
      "Could not parse email body\n",
      "Could not parse email 'from' header\n",
      "Could not parse email 'from' header\n",
      "Could not parse email 'from' header\n",
      "Could not parse email 'from' header\n",
      "Could not parse email 'from' header\n",
      "Could not parse email body\n",
      "Could not parse email 'from' header\n",
      "Could not parse email 'from' header\n",
      "Could not parse email 'from' header\n",
      "Could not parse email 'from' header\n",
      "Could not parse email 'from' header\n",
      "Could not parse email 'from' header\n",
      "Could not parse email 'from' header\n",
      "Could not parse email 'from' header\n",
      "Could not parse email 'from' header\n",
      "Could not parse email 'from' header\n",
      "Could not parse email 'from' header\n",
      "Could not parse email 'from' header\n",
      "Could not parse email 'from' header\n",
      "Could not parse email 'from' header\n",
      "Could not parse email 'from' header\n",
      "Could not parse email 'from' header\n",
      "Could not parse email 'from' header\n",
      "Could not parse email 'from' header\n",
      "Could not parse email 'from' header\n",
      "Could not parse email 'from' header\n",
      "Could not parse email 'from' header\n",
      "Could not parse email 'from' header\n",
      "Could not parse email 'from' header\n",
      "Could not parse email 'from' header\n",
      "Could not parse email 'from' header\n",
      "Could not parse email 'from' header\n",
      "Could not parse email 'from' header\n",
      "Could not parse email 'from' header\n",
      "Could not parse email 'from' header\n",
      "Could not parse email 'from' header\n",
      "Could not parse email 'from' header\n",
      "Could not parse email 'from' header\n",
      "Could not parse email 'from' header\n",
      "Could not parse email body\n",
      "Could not parse email body\n",
      "Could not parse email 'from' header\n",
      "Could not parse email body\n",
      "Could not parse email 'from' header\n",
      "Could not parse email 'from' header\n",
      "Could not parse email 'from' header\n",
      "Could not parse email 'from' header\n",
      "Could not parse email 'from' header\n",
      "Could not parse email 'from' header\n",
      "Could not parse email 'from' header\n",
      "Could not parse email body\n",
      "Could not parse email 'from' header\n",
      "Could not parse email 'from' header\n",
      "Could not parse email 'from' header\n",
      "Could not parse email body\n",
      "Could not parse email 'from' header\n",
      "Could not parse email 'from' header\n",
      "Could not parse email body\n",
      "[-] Unknown email parse error\n",
      "[-] Unknown email parse error\n",
      "Could not parse email body\n",
      "Could not parse email 'from' header\n",
      "Could not parse email 'from' header\n",
      "Could not parse email body\n",
      "Could not parse email body\n",
      "Could not parse email 'from' header\n",
      "Could not parse email body\n",
      "Could not parse email body\n",
      "Could not parse email 'from' header\n",
      "Could not parse email 'from' header\n",
      "Could not parse email 'from' header\n",
      "Could not parse email body\n",
      "[-] Unknown email parse error\n",
      "Could not parse email body\n",
      "Could not parse email body\n",
      "Could not parse email body\n",
      "Could not parse email body\n",
      "Could not parse email body\n",
      "Could not parse email body\n",
      "Could not parse email body\n",
      "Wall time: 6min 13s\n"
     ]
    }
   ],
   "source": [
    "rows_spam = parse_dataset(model_spam_files, 1)\n",
    "rows_ham = parse_dataset(model_ham_files, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "grateful-kitty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe\n",
    "rows_all = rows_spam + rows_ham\n",
    "df = pd.DataFrame(rows_all, columns = ['spam', 'raw_data', 'subject', 'from', 'urls', 'domains'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "official-cotton",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshuffle dataframes (currently spam and ham are strictly separated)\n",
    "df = sklearn.utils.shuffle(df)\n",
    "# fix indices\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressed-photograph",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add missing punkt\n",
    "#nltk.download('punkt')\n",
    "\n",
    "# add columns for amount of chars / words\n",
    "df[\"chars\"] = df[\"raw_data\"].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "vanilla-turkish",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # remove replies and forwards\n",
    "    start_reply = re.search(r\"\\nOn .* wrote:\", text)\n",
    "    if start_reply != None:\n",
    "        cleared_text = text[:start_reply.start()]\n",
    "    else:\n",
    "        cleared_text = text\n",
    "    \n",
    "    # remove \\n or \\r or \\\\n or \\\\r\n",
    "    cleared_text = cleared_text.replace('\\n', ' ').replace('\\r', ' ').replace('\\\\n', ' ').replace('\\\\r', ' ')\n",
    "    \n",
    "    # remove URLs\n",
    "    cleared_text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[-$_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', cleared_text)\n",
    "    \n",
    "    # remove email addrs\n",
    "    re_email = re.compile(\"[\\w.-]+@[\\w.-]+.[\\w.-]+\", re.UNICODE)\n",
    "    cleared_text = re_email.sub(' ', cleared_text)\n",
    "    \n",
    "    # replace non-alpha chars with space\n",
    "    cleared_text = re.sub('[^a-zA-Z]', ' ', cleared_text)\n",
    "    \n",
    "    # convert everything to lowercase\n",
    "    cleared_text = cleared_text.lower()\n",
    "    \n",
    "    cleared_text = cleared_text.split()\n",
    "    cleared_text = ' '.join(cleared_text)\n",
    "    \n",
    "    return cleared_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "important-bermuda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new column for cleaned data (no urls, email addrs, only alpha, all lowercase)\n",
    "df[\"data\"] = df[\"raw_data\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "celtic-rainbow",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"token_text\"] = df.apply(lambda row: nltk.word_tokenize(str(row[\"data\"])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-norman",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens']  = df['token_text'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorrect-asthma",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    filtered_text = [word for word in text if word not in stop_words]\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "terminal-hearts",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"stop_text\"] = df[\"token_text\"].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-association",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_word(text):\n",
    "    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in text]\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adequate-representation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"clean_text\"] = df[\"stop_text\"].apply(lemmatize_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-shuttle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "df = df.drop_duplicates(subset=\"raw_data\", keep=\"first\")\n",
    "\n",
    "# Remove emails with less then five words\n",
    "print(f\"All emails without dups: {len(df)}\")\n",
    "print(f\"Emails with less then five words: {len(df[df['clean_text'].map(lambda d: len(d)) < 5])}\")\n",
    "df = df.drop(df[df['clean_text'].map(lambda d: len(d)) < 5].index)\n",
    "print(f\"Emails with at least 5 words: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dense-philosophy",
   "metadata": {},
   "source": [
    "# Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soviet-macro",
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_words = set()\n",
    "for text in df[\"clean_text\"]:\n",
    "    for word in text:\n",
    "        collected_words.add(word)\n",
    "\n",
    "wordlist_nltk = words.words()\n",
    "\n",
    "wordlist_github = [] # https://github.com/dwyl/english-words\n",
    "with open(\"resources/words-dwyl-github.txt\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        wordlist_github.append(line[:-1])\n",
    "\n",
    "wordlist_github_nltk_combined = wordlist_github + wordlist_nltk\n",
    "\n",
    "wordlist_github_nltk_combined_lower = [x.lower() for x in wordlist_github_nltk_combined]\n",
    "intersection_github_nltk_combined_lower = collected_words.intersection(wordlist_github_nltk_combined_lower)\n",
    "vocabulary = intersection_github_nltk_combined_lower\n",
    "len(vocabulary)\n",
    "\n",
    "joblib.dump(vocabulary, \"exports/vocab.sav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-belfast",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "exterior-shell",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spam = df[df[\"spam\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "knowing-median",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ham = df[df[\"spam\"] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "banned-diamond",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use 25k for balanced dataset\n",
    "amount = 25000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "macro-discipline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshuffle dataframes (currently spam and ham are strictly separated)\n",
    "df_spam = sklearn.utils.shuffle(df_spam)\n",
    "df_ham = sklearn.utils.shuffle(df_ham)\n",
    "\n",
    "# fix indices\n",
    "df_spam = df_spam.reset_index(drop=True)\n",
    "df_ham = df_ham.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "checked-handy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create balanced df (25k spam and 25k ham mails) for training the model\n",
    "df_model_train = df_spam[:amount].append(df_ham[:amount])\n",
    "df_model_train = sklearn.utils.shuffle(df_model_train)\n",
    "df_model_train = df_model_train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "southeast-newcastle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the rest of df_spam and df_ham but keep the new df (df_model_test) balanced\n",
    "df_model_test = df_spam[amount:28410].append(df_ham[amount:28410])\n",
    "df_model_test = sklearn.utils.shuffle(df_model_test)\n",
    "df_model_test = df_model_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "according-shopper",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for text in df_model_train[\"clean_text\"]:\n",
    "    msg = ' '.join([row for row in text])\n",
    "    corpus.append(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "subsequent-diana",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(vocabulary=vocabulary)\n",
    "x_train = tfidf.fit_transform(corpus).toarray()\n",
    "x_train = np.append(x_train, df_model_train[[\"tokens\", \"chars\"]].to_numpy(), axis=1)\n",
    "y_train = df_model_train[\"spam\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "other-clock",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for text in df_model_test[\"clean_text\"]:\n",
    "    msg = ' '.join([row for row in text])\n",
    "    corpus.append(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "juvenile-diagram",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = tfidf.fit_transform(corpus).toarray()\n",
    "x_test = np.append(x_test, df_model_test[[\"tokens\", \"chars\"]].to_numpy(), axis=1)\n",
    "y_test = df_model_test[\"spam\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "moving-singing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator: RandomForestClassifier()\n",
      "Train-Duration: 498.00077414512634 s\n",
      "Accuracy: 0.9787390029325513 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "classifiers = [RandomForestClassifier()]\n",
    "\n",
    "for cls in classifiers:\n",
    "    time_start = time.time()\n",
    "    cls.fit(x_train, y_train)\n",
    "    time_end = time.time()\n",
    "    print(\"Estimator:\", cls)\n",
    "    print(\"Train-Duration:\", time_end - time_start, \"s\")\n",
    "    score = cls.score(x_test, y_test)\n",
    "    print(\"Accuracy:\", score, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "alive-keyboard",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['exports/model.sav']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# export model\n",
    "joblib.dump(classifiers[0], \"exports/model.sav\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
