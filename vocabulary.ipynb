{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "overall-china",
   "metadata": {},
   "source": [
    "# Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "respective-extension",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import json\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import email\n",
    "import urllib\n",
    "import base64\n",
    "import string\n",
    "import quopri\n",
    "\n",
    "import seaborn as sns\n",
    "import sklearn.utils\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import words\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline    \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import precision_score, recall_score, plot_confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "encouraging-ethernet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load clean dataframe\n",
    "df = joblib.load(\"exports/dataframe.sav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "egyptian-formula",
   "metadata": {},
   "source": [
    "## Finding the best vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "hundred-alaska",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to have the biggest vocabulary (but only valid words) possible.\n",
    "# This is needed because after training our model the vocabulary is fixed and can not be changed anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "banned-equipment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All words of a huge wordlist can not be used because this would take way too long and take too many ressources (huge X matrix).\n",
    "# The idea / tradeoff is to use all words from our known emails from the given datasets and remove all invalid ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "monetary-victor",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "vocabulary = set()\n",
    "for text in df[\"clean_text\"]:\n",
    "    msg = ' '.join([row for row in text])\n",
    "    corpus.append(msg)\n",
    "    for word in text:\n",
    "        vocabulary.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "certified-strengthening",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221767"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "innovative-turkish",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nlri',\n",
       " 'ijyyjsigawq',\n",
       " 'rucm',\n",
       " 'tstudent',\n",
       " 'bajativos',\n",
       " 'schitannye',\n",
       " 'drozhi',\n",
       " 'liegrow',\n",
       " 'resalable',\n",
       " 'vynhnbucqlr']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vocabulary)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "egyptian-sympathy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are strings in our vocabulary that are not valid (english) words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "elder-spread",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to remove only unvalid words (keep as many valid words as possible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "worldwide-hospital",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idea: try different wordlists (and combine them) and make a intersection with the found words of the emails\n",
    "# --> as much valid words as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "voluntary-vampire",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29142"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlist_nltk = words.words()\n",
    "intersection_nltk = vocabulary.intersection(wordlist_nltk)\n",
    "len(intersection_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "focal-olympus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with the nltk lib wordlist we get 29142 valid words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "opponent-amendment",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\Konstantin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18111"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('brown')\n",
    "from nltk.corpus import brown\n",
    "wordlist_brown = brown.words()\n",
    "intersection_brown = vocabulary.intersection(wordlist_brown)\n",
    "len(intersection_brown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "brave-casino",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the brown wordlist removes more words, probably because this wordlist is pretty old (contains words that are not used anymore / in emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "wooden-protest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36945"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlist_github = [] # https://github.com/dwyl/english-words\n",
    "with open(\"resources/words-dwyl-github.txt\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        wordlist_github.append(line[:-1])\n",
    "\n",
    "intersection_github = vocabulary.intersection(wordlist_github)\n",
    "len(intersection_github)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "prescribed-mediterranean",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the github wordlist keeps more words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "federal-smell",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41012"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlist_github_nltk_combined = wordlist_github + wordlist_nltk\n",
    "intersection_github_nltk_combined = vocabulary.intersection(wordlist_github_nltk_combined)\n",
    "len(intersection_github_nltk_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "historic-novel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining the nltk and the github wordlist seems to be the best approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "metropolitan-spell",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55305"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlist_github_nltk_combined_lower = [x.lower() for x in wordlist_github_nltk_combined]\n",
    "intersection_github_nltk_combined_lower = vocabulary.intersection(wordlist_github_nltk_combined_lower)\n",
    "len(intersection_github_nltk_combined_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "small-realtor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we get more words because we've lowered the choosen wordlist\n",
    "# this is needed because our preprocessed text is only lowercase\n",
    "# (words like \"Example\" in the wordlist will not be in the intersection because the vocabulary only contains lowercase words like \"example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "efficient-artwork",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104119\n"
     ]
    }
   ],
   "source": [
    "uppercase_words = 0\n",
    "for x in wordlist_github_nltk_combined:\n",
    "    if x[0] in \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\":\n",
    "        #print(x)\n",
    "        uppercase_words += 1\n",
    "print(uppercase_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "hairy-environment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "uppercase_words = 0\n",
    "for x in wordlist_github_nltk_combined_lower:\n",
    "    if x[0] in \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\":\n",
    "        #print(x)\n",
    "        uppercase_words += 1\n",
    "print(uppercase_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "blocked-charles",
   "metadata": {},
   "outputs": [],
   "source": [
    "sym_diff_github_nltk_combined = sorted(intersection_github_nltk_combined_lower.symmetric_difference(intersection_github_nltk_combined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "intended-laptop",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14293"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many uppercase words have been added due to lowering the words of the set\n",
    "len(sym_diff_github_nltk_combined)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
